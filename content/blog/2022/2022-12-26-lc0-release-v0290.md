---
title: "Lc0 release v0.29.0"
# slug: "blog-post-title"
published: 2022-12-26
#draft: true
author: "borg"
# image: "name"
# cover: "cover.png" # Default
---

After a full year without a Lc0 release, [v0.29.0](https://github.com/LeelaChessZero/lc0/releases/tag/v0.29.0) was finally released a few weeks ago, so this is a perfect opportunity to make a blog post about it and explain why it took so long. Moreover, this way we'll get at least one blog post in 2022...
<!--more-->

In the following list I have broken down and rearranged the bullet list from the release announcement in several categories with more detailed descriptions.

### Attention policy support.
* Full attention policy support in cuda, cudnn, metal, onnx, blas, dnnl, and eigen backends.
* Partial attention policy support in onednn backend (good enough for T79).
* The default net is now 791556 for most backends except opencl and dx12 that get 753723 (as they lack attention policy support).

This is the main reason this release was delayed. We have started experimenting with new neural network architectures and one of the first imrovements is called attention policy. The policy output of the neural net, for a given position, gives the probability each legal move is the best one. Attention policy is a new way to calculate this, giving more accurate results, but requires modifications of the code that does the neural net computation. This code is broken down in several backends, each one tagetting a specific mentod of computation, either specific gpus or external libraries. Unfortunately, only a few members of our team are able to do for those performance critial bits of code and had limited time available - this is a volunteer project after all. We have finally working solutions that cover most of our users and an approach that can hopefully allow us to have support for new architectures a lot quicker - more on that later.

Currently all recent network series (T78, T79 and T80) require this support, meaning that the backends that haven't been updated (dx12 and opencl) do not work with those nets. Further, the T78 nets require some of the optional attention policy features and as such are usually the last to be supported: the onednn backend is not yet at this point, but can be used with all other network architectures. 

### New and improved backends
* New metal backend for apple systems. This is now the default backend for macos builds.

The new metal backend was frequently requested by macos users. This is faster than opencl, the best that was previously available for macs, and moreover supports the newer nets. The downside is that it currently only works with newer macos releases, but we expect we will be able to support earlier versions soon.

* New onnx-dml backend to use DirectML under windows, has better net compatibility than dx12 and is faster than opencl. See the README for use instructions, a separate download of the DirectML dll is required.
* Now the onnx backends can use fp16 when running with a network file (not with .onnx model files). This is the default for onnx-cuda and onnx-dml, can be switched on or off with by setting the `fp16` backend option to `true` or `false` respectively.
* The onnx backend now allows selecting gpu to use.

The onnx backend gets a new flavor (onnx-dml) in addition to the existing ones (onnx-cuda and onnx-cpu). This new variation uses windows' DirectML library to handle the computation, working on every system our previous dx12 backend supporteed and more. While it is a bit slower than the dx12 backend (for now), it supports the new networs and their improved strength more than overcomes this speed loss. To run onnx-dml a very recent DirectML dll is required, and can be dowloaded from <https://www.nuget.org/api/v2/package/Microsoft.AI.DirectML/1.10.0>. This is a nuget installer package, but can be used as a normal zip file by changing the extension to .zip - the dll needed is `/bin/x64-win/DirectML.dll`.

Finally, the onnx backend is quite easy to expand, so we hope it will offer a way to support new network architectures (currently under development) in a timely manner - having a baseline working backend is a huge help for working and updating other backends.

* The onednn package comes with the latest dnnl compiled to allow running on an intel gpu by adding `gpu=0` to the backend options.

### Miscelaneous
* Support for using pgn book with long lines in training: selfplay can start at a random point in the book.

The plan is to use a book with long opening lines (or even full games) for training experiments, where the selfplay games deviate from random points of each line.

* Support for double Fischer random chess (dfrc).

Double Fischer random chess is a variation of Fischer random chess (FRC - also known as chess 960) where the opening position is not necessarilly symmetric - this is a huge superset of FRC.

* New "simple" time manager.
* Added TC-dependent output to the backendbench assistant.
* Starting with this version, the check backend compares policy for valid moves after softmax.
* Improved error messages for unsupported network files.
* Non multigather (legacy) search code and `--multigather` option are removed.
* Some assorted fixes and code cleanups.

